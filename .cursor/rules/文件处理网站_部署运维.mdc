---
description: 
globs: 
alwaysApply: false
---
# 文件处理网站部署运维文档

## 1. 环境准备

### 1.1 系统要求

#### 1.1.1 硬件要求
- **CPU**：8核心以上 (建议16核心)
- **内存**：16GB以上 (建议32GB)
- **存储**：SSD 500GB以上
- **GPU**：NVIDIA GPU (建议RTX 4090或A100)
- **网络**：千兆网卡

#### 1.1.2 软件要求
- **操作系统**：Ubuntu 20.04+ / CentOS 8+
- **Docker**：20.10+
- **Docker Compose**：2.0+
- **Python**：3.9+
- **Node.js**：18+
- **Redis**：6.0+
- **Nginx**：1.20+

### 1.2 依赖服务安装

#### 1.2.1 Docker安装
```bash
# Ubuntu系统
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# 安装Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

#### 1.2.2 NVIDIA Docker支持
```bash
# 安装NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

## 2. 项目部署

### 2.1 项目结构
```
file_processing_website/
├── docker/
│   ├── docker-compose.yml           # 完整服务编排
│   ├── docker-compose.dev.yml       # 开发环境
│   ├── docker-compose.prod.yml      # 生产环境
│   ├── Dockerfile.backend           # 后端镜像
│   ├── Dockerfile.frontend          # 前端镜像
│   ├── Dockerfile.worker            # 任务处理器镜像
│   └── nginx/
│       ├── nginx.conf               # Nginx配置
│       └── ssl/                     # SSL证书
├── backend/
├── frontend/
├── scripts/
│   ├── deploy.sh                    # 部署脚本
│   ├── backup.sh                    # 备份脚本
│   └── monitor.sh                   # 监控脚本
└── .env                             # 环境变量
```

### 2.2 Docker配置文件

#### 2.2.1 后端Dockerfile
```dockerfile
# docker/Dockerfile.backend
FROM nvidia/cuda:11.8-devel-ubuntu20.04

# 设置时区
ENV TZ=Asia/Shanghai
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    python3.9 \
    python3.9-pip \
    python3.9-dev \
    git \
    wget \
    curl \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# 设置工作目录
WORKDIR /app

# 复制依赖文件
COPY backend/requirements.txt .

# 安装Python依赖
RUN pip3 install --no-cache-dir -r requirements.txt

# 复制应用代码
COPY backend/ .
COPY magic_pdf/ ./magic_pdf/

# 创建存储目录
RUN mkdir -p storage/uploads storage/processing storage/results

# 设置权限
RUN chmod +x scripts/*.sh

# 暴露端口
EXPOSE 8000

# 启动命令
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### 2.2.2 前端Dockerfile
```dockerfile
# docker/Dockerfile.frontend
FROM node:18-alpine AS builder

WORKDIR /app

# 复制package文件
COPY frontend/package*.json ./

# 安装依赖
RUN npm ci --only=production

# 复制源码
COPY frontend/ .

# 构建应用
RUN npm run build

# 生产镜像
FROM nginx:alpine

# 复制构建结果
COPY --from=builder /app/dist /usr/share/nginx/html

# 复制Nginx配置
COPY docker/nginx/nginx.conf /etc/nginx/nginx.conf

# 暴露端口
EXPOSE 80 443

CMD ["nginx", "-g", "daemon off;"]
```

#### 2.2.3 任务处理器Dockerfile
```dockerfile
# docker/Dockerfile.worker
FROM nvidia/cuda:11.8-devel-ubuntu20.04

# 继承后端镜像的配置
FROM file_processing_website_backend:latest

# 安装Celery
RUN pip3 install celery[redis]

# 启动Celery Worker
CMD ["celery", "-A", "app.celery_app", "worker", "--loglevel=info", "--concurrency=2"]
```

### 2.3 Docker Compose配置

#### 2.3.1 生产环境配置
```yaml
# docker/docker-compose.prod.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: file_processor_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru

  backend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.backend
    container_name: file_processor_backend
    restart: unless-stopped
    depends_on:
      - redis
    ports:
      - "8000:8000"
    volumes:
      - ../storage:/app/storage
      - ../model_weight:/app/model_weight
    environment:
      - REDIS_URL=redis://redis:6379/0
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  worker:
    build:
      context: ..
      dockerfile: docker/Dockerfile.worker
    container_name: file_processor_worker
    restart: unless-stopped
    depends_on:
      - redis
      - backend
    volumes:
      - ../storage:/app/storage
      - ../model_weight:/app/model_weight
    environment:
      - REDIS_URL=redis://redis:6379/0
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  frontend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.frontend
    container_name: file_processor_frontend
    restart: unless-stopped
    depends_on:
      - backend
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/ssl:/etc/nginx/ssl

  flower:
    image: mher/flower:0.9.7
    container_name: file_processor_flower
    restart: unless-stopped
    depends_on:
      - redis
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - FLOWER_PORT=5555

volumes:
  redis_data:

networks:
  default:
    name: file_processor_network
```

#### 2.3.2 开发环境配置
```yaml
# docker/docker-compose.dev.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  backend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.backend
    ports:
      - "8000:8000"
    volumes:
      - ..:/app
      - ../storage:/app/storage
    environment:
      - REDIS_URL=redis://redis:6379/0
      - DEBUG=true
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  frontend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.frontend
      target: builder
    ports:
      - "3000:3000"
    volumes:
      - ../frontend:/app
    command: npm run dev
```

### 2.4 Nginx配置

#### 2.4.1 主配置文件
```nginx
# docker/nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # 日志格式
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                   '$status $body_bytes_sent "$http_referer" '
                   '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # 基础配置
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_tokens off;

    # 文件上传限制
    client_max_body_size 200M;
    client_body_timeout 60s;
    client_header_timeout 60s;

    # Gzip压缩
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;

    # 上游服务器
    upstream backend {
        server backend:8000;
        keepalive 32;
    }

    # HTTP重定向到HTTPS
    server {
        listen 80;
        server_name _;
        return 301 https://$server_name$request_uri;
    }

    # HTTPS主服务器
    server {
        listen 443 ssl http2;
        server_name your-domain.com;

        # SSL配置
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_session_timeout 1d;
        ssl_session_cache shared:SSL:50m;
        ssl_session_tickets off;

        # 现代SSL配置
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256;
        ssl_prefer_server_ciphers off;

        # HSTS
        add_header Strict-Transport-Security "max-age=63072000" always;

        # 静态文件
        location / {
            root /usr/share/nginx/html;
            index index.html;
            try_files $uri $uri/ /index.html;
            
            # 缓存控制
            location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
                expires 1y;
                add_header Cache-Control "public, immutable";
            }
        }

        # API代理
        location /api/ {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # 超时设置
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 300s;
            
            # 缓冲设置
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }

        # WebSocket代理
        location /ws/ {
            proxy_pass http://backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # WebSocket超时
            proxy_read_timeout 86400;
        }

        # 文件下载
        location /download/ {
            internal;
            alias /app/storage/results/;
            
            # 下载文件名处理
            add_header Content-Disposition 'attachment; filename="$arg_filename"';
        }
    }
}
```

## 3. 部署脚本

### 3.1 自动化部署脚本
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

# 配置变量
PROJECT_NAME="file_processing_website"
DEPLOY_ENV=${1:-prod}
BACKUP_DIR="/backup/$(date +%Y%m%d_%H%M%S)"

echo "开始部署 $PROJECT_NAME ($DEPLOY_ENV 环境)"

# 创建备份
echo "创建备份..."
mkdir -p $BACKUP_DIR
docker-compose -f docker/docker-compose.$DEPLOY_ENV.yml down
cp -r storage $BACKUP_DIR/
cp -r model_weight $BACKUP_DIR/

# 拉取最新代码
echo "拉取最新代码..."
git pull origin main

# 构建镜像
echo "构建Docker镜像..."
docker-compose -f docker/docker-compose.$DEPLOY_ENV.yml build --no-cache

# 启动服务
echo "启动服务..."
docker-compose -f docker/docker-compose.$DEPLOY_ENV.yml up -d

# 等待服务启动
echo "等待服务启动..."
sleep 30

# 健康检查
echo "执行健康检查..."
if curl -f http://localhost:8000/health; then
    echo "部署成功！"
else
    echo "部署失败，回滚到备份..."
    docker-compose -f docker/docker-compose.$DEPLOY_ENV.yml down
    cp -r $BACKUP_DIR/storage ./
    cp -r $BACKUP_DIR/model_weight ./
    docker-compose -f docker/docker-compose.$DEPLOY_ENV.yml up -d
    exit 1
fi

# 清理旧镜像
echo "清理旧镜像..."
docker image prune -f

echo "部署完成！"
```

### 3.2 备份脚本
```bash
#!/bin/bash
# scripts/backup.sh

set -e

BACKUP_BASE_DIR="/backup"
RETENTION_DAYS=7
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="$BACKUP_BASE_DIR/$DATE"

echo "开始备份..."

# 创建备份目录
mkdir -p $BACKUP_DIR

# 备份存储文件
echo "备份存储文件..."
tar -czf $BACKUP_DIR/storage.tar.gz storage/

# 备份Redis数据
echo "备份Redis数据..."
docker exec file_processor_redis redis-cli BGSAVE
sleep 5
docker cp file_processor_redis:/data/dump.rdb $BACKUP_DIR/

# 备份配置文件
echo "备份配置文件..."
cp -r docker/ $BACKUP_DIR/
cp .env $BACKUP_DIR/

# 创建备份信息文件
echo "创建备份信息..."
cat > $BACKUP_DIR/backup_info.txt << EOF
备份时间: $(date)
备份类型: 完整备份
Git提交: $(git rev-parse HEAD)
Docker镜像:
$(docker images --format "table {{.Repository}}:{{.Tag}}\t{{.ID}}\t{{.Size}}" | grep file_processor)
EOF

# 清理过期备份
echo "清理过期备份..."
find $BACKUP_BASE_DIR -type d -mtime +$RETENTION_DAYS -exec rm -rf {} \;

echo "备份完成: $BACKUP_DIR"
```

## 4. 监控告警

### 4.1 系统监控脚本
```bash
#!/bin/bash
# scripts/monitor.sh

# 监控配置
CPU_THRESHOLD=80
MEMORY_THRESHOLD=80
DISK_THRESHOLD=85
GPU_MEMORY_THRESHOLD=90

# 获取系统指标
get_cpu_usage() {
    top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1
}

get_memory_usage() {
    free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}'
}

get_disk_usage() {
    df -h / | awk 'NR==2{print $5}' | cut -d'%' -f1
}

get_gpu_memory_usage() {
    nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits | \
    awk -F', ' '{printf "%.1f", $1/$2 * 100.0}'
}

# 检查Docker容器状态
check_containers() {
    local unhealthy_containers=$(docker ps --filter "health=unhealthy" --format "{{.Names}}")
    if [ ! -z "$unhealthy_containers" ]; then
        echo "ALERT: 发现不健康的容器: $unhealthy_containers"
        return 1
    fi
    return 0
}

# 检查服务可用性
check_services() {
    # 检查后端API
    if ! curl -f -s http://localhost:8000/health > /dev/null; then
        echo "ALERT: 后端API服务不可用"
        return 1
    fi
    
    # 检查前端
    if ! curl -f -s http://localhost/ > /dev/null; then
        echo "ALERT: 前端服务不可用"
        return 1
    fi
    
    # 检查Redis
    if ! docker exec file_processor_redis redis-cli ping > /dev/null; then
        echo "ALERT: Redis服务不可用"
        return 1
    fi
    
    return 0
}

# 发送告警
send_alert() {
    local message=$1
    echo "$(date): $message" >> /var/log/file_processor_alerts.log
    
    # 发送邮件告警（需要配置邮件服务）
    # echo "$message" | mail -s "文件处理网站告警" admin@example.com
    
    # 发送到Slack（需要配置Webhook）
    # curl -X POST -H 'Content-type: application/json' \
    #   --data "{\"text\":\"$message\"}" \
    #   $SLACK_WEBHOOK_URL
}

# 主监控逻辑
main() {
    echo "开始系统监控检查..."
    
    # 检查CPU使用率
    cpu_usage=$(get_cpu_usage)
    if (( $(echo "$cpu_usage > $CPU_THRESHOLD" | bc -l) )); then
        send_alert "CPU使用率过高: ${cpu_usage}%"
    fi
    
    # 检查内存使用率
    memory_usage=$(get_memory_usage)
    if (( $(echo "$memory_usage > $MEMORY_THRESHOLD" | bc -l) )); then
        send_alert "内存使用率过高: ${memory_usage}%"
    fi
    
    # 检查磁盘使用率
    disk_usage=$(get_disk_usage)
    if [ $disk_usage -gt $DISK_THRESHOLD ]; then
        send_alert "磁盘使用率过高: ${disk_usage}%"
    fi
    
    # 检查GPU内存使用率
    if command -v nvidia-smi &> /dev/null; then
        gpu_memory_usage=$(get_gpu_memory_usage)
        if (( $(echo "$gpu_memory_usage > $GPU_MEMORY_THRESHOLD" | bc -l) )); then
            send_alert "GPU内存使用率过高: ${gpu_memory_usage}%"
        fi
    fi
    
    # 检查容器状态
    if ! check_containers; then
        send_alert "发现不健康的Docker容器"
    fi
    
    # 检查服务可用性
    if ! check_services; then
        send_alert "服务可用性检查失败"
    fi
    
    echo "监控检查完成"
}

# 设置定时任务
# crontab -e
# */5 * * * * /path/to/scripts/monitor.sh

main "$@"
```

### 4.2 Prometheus监控配置
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'nginx-exporter'
    static_configs:
      - targets: ['nginx-exporter:9113']

  - job_name: 'file-processor-backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
```

### 4.3 Grafana仪表板配置
```json
{
  "dashboard": {
    "title": "文件处理网站监控",
    "panels": [
      {
        "title": "系统资源使用率",
        "type": "stat",
        "targets": [
          {
            "expr": "100 - (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU使用率"
          },
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
            "legendFormat": "内存使用率"
          }
        ]
      },
      {
        "title": "文件处理任务统计",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(file_processor_tasks_total[5m])",
            "legendFormat": "任务处理速率"
          },
          {
            "expr": "file_processor_tasks_pending",
            "legendFormat": "待处理任务"
          }
        ]
      },
      {
        "title": "API响应时间",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) * 1000",
            "legendFormat": "95th百分位响应时间"
          }
        ]
      }
    ]
  }
}
```

## 5. 日志管理

### 5.1 日志配置
```python
# backend/app/core/logging_config.py
import logging
import sys
from pathlib import Path
from loguru import logger

# 移除默认处理器
logger.remove()

# 控制台输出
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO"
)

# 文件输出
log_dir = Path("/app/logs")
log_dir.mkdir(exist_ok=True)

# 应用日志
logger.add(
    log_dir / "app.log",
    rotation="100 MB",
    retention="30 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"
)

# 错误日志
logger.add(
    log_dir / "error.log",
    rotation="100 MB",
    retention="30 days",
    level="ERROR",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"
)

# 任务处理日志
logger.add(
    log_dir / "tasks.log",
    rotation="100 MB",
    retention="30 days",
    level="INFO",
    filter=lambda record: "task" in record["extra"],
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | TASK:{extra[task_id]} - {message}"
)
```

### 5.2 ELK日志收集
```yaml
# monitoring/docker-compose.elk.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    container_name: logstash
    ports:
      - "5044:5044"
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.8.0
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ../logs:/app/logs:ro
    depends_on:
      - logstash

volumes:
  elasticsearch_data:
```

## 6. 安全加固

### 6.1 防火墙配置
```bash
#!/bin/bash
# scripts/firewall.sh

# 清空现有规则
iptables -F
iptables -X
iptables -t nat -F
iptables -t nat -X

# 设置默认策略
iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT

# 允许本地回环
iptables -A INPUT -i lo -j ACCEPT

# 允许已建立的连接
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

# 允许SSH
iptables -A INPUT -p tcp --dport 22 -j ACCEPT

# 允许HTTP/HTTPS
iptables -A INPUT -p tcp --dport 80 -j ACCEPT
iptables -A INPUT -p tcp --dport 443 -j ACCEPT

# 允许监控端口（仅内网）
iptables -A INPUT -p tcp -s 10.0.0.0/8 --dport 9090 -j ACCEPT
iptables -A INPUT -p tcp -s 10.0.0.0/8 --dport 3000 -j ACCEPT

# 限制连接速率
iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT
iptables -A INPUT -p tcp --dport 443 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT

# 保存规则
iptables-save > /etc/iptables/rules.v4
```

### 6.2 SSL证书自动更新
```bash
#!/bin/bash
# scripts/renew_ssl.sh

# Let's Encrypt自动续期
certbot renew --quiet --no-self-upgrade

# 重新加载Nginx
docker exec file_processor_frontend nginx -s reload

# 记录日志
echo "$(date): SSL证书检查完成" >> /var/log/ssl_renewal.log
```

## 7. 故障排除

### 7.1 常见问题诊断
```bash
#!/bin/bash
# scripts/diagnose.sh

echo "=== 文件处理网站故障诊断 ==="

# 检查Docker服务
echo "1. 检查Docker服务状态..."
systemctl status docker

# 检查容器状态
echo "2. 检查容器状态..."
docker ps -a

# 检查容器日志
echo "3. 检查容器日志..."
for container in file_processor_backend file_processor_frontend file_processor_worker file_processor_redis; do
    echo "--- $container 日志 ---"
    docker logs --tail 50 $container
done

# 检查磁盘空间
echo "4. 检查磁盘空间..."
df -h

# 检查内存使用
echo "5. 检查内存使用..."
free -h

# 检查GPU状态
echo "6. 检查GPU状态..."
nvidia-smi

# 检查网络连接
echo "7. 检查网络连接..."
netstat -tlnp | grep -E ':(80|443|8000|6379)'

# 检查文件权限
echo "8. 检查文件权限..."
ls -la storage/

echo "=== 诊断完成 ==="
```

### 7.2 数据恢复脚本
```bash
#!/bin/bash
# scripts/restore.sh

BACKUP_DATE=$1
BACKUP_DIR="/backup/$BACKUP_DATE"

if [ -z "$BACKUP_DATE" ]; then
    echo "用法: $0 <备份日期>"
    echo "可用备份:"
    ls -la /backup/
    exit 1
fi

if [ ! -d "$BACKUP_DIR" ]; then
    echo "备份目录不存在: $BACKUP_DIR"
    exit 1
fi

echo "开始恢复数据..."

# 停止服务
docker-compose -f docker/docker-compose.prod.yml down

# 恢复存储文件
echo "恢复存储文件..."
rm -rf storage/
tar -xzf $BACKUP_DIR/storage.tar.gz

# 恢复Redis数据
echo "恢复Redis数据..."
docker run --rm -v redis_data:/data -v $BACKUP_DIR:/backup alpine cp /backup/dump.rdb /data/

# 恢复配置文件
echo "恢复配置文件..."
cp -r $BACKUP_DIR/docker/ ./
cp $BACKUP_DIR/.env ./

# 启动服务
echo "启动服务..."
docker-compose -f docker/docker-compose.prod.yml up -d

echo "数据恢复完成！"
```

---

**文档版本**：v1.0  
**制定时间**：2025年1月  
**适用范围**：文件处理网站项目  
**维护者**：运维团队
