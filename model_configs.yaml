device: cuda # cuda / cpu / mps (using `transformers` as backend)
weights:
  doclayout_yolo: Structure/doclayout_yolo_docstructbench_imgsz1280_2501.pt # or Structure/layout_zh.pt
  layoutreader: Relation
models_dir: model_weight
layout_config: 
  model: doclayout_yolo # PP-DocLayout_plus-L / doclayout_yolo
  reader:
    name: layoutreader
chat_config:
  weight_path: model_weight/Recognition
  backend: transformers # lmdeploy / vllm / transformers / api / lmdeploy_queue / vllm_queue
  batch_size: 3 # 降低batch_size以减少内存使用，兼容RTX 2080 Ti
  use_flash_attention: false # 明确禁用FlashAttention，兼容RTX 2080 Ti (Turing架构)
  attn_implementation: eager # 禁用FlashAttention，使用标准attention
  use_flash_attention_2: false # 明确禁用FlashAttention2
  torch_dtype: float16 # 使用float16减少显存占用
  # if using xxx_queue as backend
  queue_config:
    max_batch_size: 256 # maximum batch size for internal processing
    queue_timeout: 1 # seconds to wait for batching requests
    max_queue_size: 2000 # maximum requests in queue

# Uncomment the following lines if use `api` as backend 
# api_config:
#   url: https://api.openai.com/v1
#   model_name: gpt-4.1
#   api_key: sk-xxx
